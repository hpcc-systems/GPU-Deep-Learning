{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset pre-processing for HPCC Systems\n",
    "## Some datasets require some slight modification before they can be easliy sprayed onto an HPCC Systems Platform cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST\n",
    "Uses the same format as MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Fashion MNIST\n",
    "#strip the first bytes and combine the labels and data\n",
    "#outputs 2 files, a test and training dataset\n",
    "#the same code will work for regular MNIST too\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dir = 'fashion_mnist/'\n",
    "testFiles = ['t10k-images-idx3-ubyte','t10k-labels-idx1-ubyte']\n",
    "trainFiles = ['train-images-idx3-ubyte','train-labels-idx1-ubyte']\n",
    "\n",
    "newTestFile = 'fashion_mnist_test_noheader'\n",
    "newTrainFile = 'fashion_mnist_train_noheader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read32(bytestream):\n",
    "  dt = np.dtype(np.uint32).newbyteorder('>')\n",
    "  return np.frombuffer(bytestream.read(4), dtype=dt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLabels(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        magicNum = _read32(f)\n",
    "        numItems = _read32(f)\n",
    "        buf = f.read(numItems)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "def readImages(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        magicNum = _read32(f)\n",
    "        num_images = _read32(f)\n",
    "        rows = _read32(f)\n",
    "        cols = _read32(f)\n",
    "        buf = f.read(rows * cols * num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8)\n",
    "        data = data.reshape(num_images, 784)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeNewFile(file, labels, images):\n",
    "    with open(file, 'wb') as nf:\n",
    "        for i in range(len(labels)):\n",
    "            nf.write(labels[i])\n",
    "            nf.write(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabels = readLabels(dir+testFiles[1])\n",
    "trainLabels = readLabels(dir+trainFiles[1])\n",
    "testImages = readImages(dir + testFiles[0])\n",
    "trainImages = readImages(dir + trainFiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(testLabels.shape)\n",
    "print(trainLabels.shape)\n",
    "print(testImages.shape)\n",
    "print(trainImages.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeNewFile(dir+newTestFile, testLabels, testImages)\n",
    "writeNewFile(dir+newTrainFile, trainLabels, trainImages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'mnist/'\n",
    "testFiles = ['t10k-images-idx3-ubyte','t10k-labels-idx1-ubyte']\n",
    "trainFiles = ['train-images-idx3-ubyte','train-labels-idx1-ubyte']\n",
    "\n",
    "newTestFile = 'mnist_test_noheader'\n",
    "newTrainFile = 'mnist_train_noheader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabels = readLabels(dir+testFiles[1])\n",
    "trainLabels = readLabels(dir+trainFiles[1])\n",
    "testImages = readImages(dir + testFiles[0])\n",
    "trainImages = readImages(dir + trainFiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeNewFile(dir+newTestFile, testLabels, testImages)\n",
    "writeNewFile(dir+newTrainFile, trainLabels, trainImages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIG MNIST\n",
    "Makes MNIST dataset arbitrarily large for testing the memory limitations within HPCC/ECL code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'mnist/'\n",
    "testFiles = ['t10k-images-idx3-ubyte','t10k-labels-idx1-ubyte']\n",
    "trainFiles = ['train-images-idx3-ubyte','train-labels-idx1-ubyte']\n",
    "\n",
    "newTestFile_big = 'mnist_test_noheader_big'\n",
    "newTrainFile_big = 'mnist_train_noheader_big'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Shapes:  (10000,) (60000,) (10000, 784) (60000, 784)\n"
     ]
    }
   ],
   "source": [
    "testLabels = readLabels(dir+testFiles[1])\n",
    "trainLabels = readLabels(dir+trainFiles[1])\n",
    "testImages = readImages(dir + testFiles[0])\n",
    "trainImages = readImages(dir + trainFiles[0])\n",
    "\n",
    "print('Base Shapes: ', testLabels.shape, trainLabels.shape, testImages.shape, trainImages.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabels_big = testLabels\n",
    "trainLabels_big = trainLabels\n",
    "testImages_big = testImages\n",
    "trainImages_big = trainImages\n",
    "\n",
    "# multiplier of 100 produces roughly 750 mb test file and 4.4 GB train file\n",
    "multiplier = 9 \n",
    "for _ in range(multiplier - 1):\n",
    "    testLabels_big = np.append(testLabels_big, testLabels)\n",
    "    trainLabels_big = np.append(trainLabels_big, trainLabels)\n",
    "    testImages_big = np.append(testImages_big, testImages, axis=0)\n",
    "    trainImages_big = np.append(trainImages_big, trainImages, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shapes after resize:  (90000,) (540000,) (90000, 784) (540000, 784)\n"
     ]
    }
   ],
   "source": [
    "print('Final shapes after resize: ', testLabels_big.shape, trainLabels_big.shape, testImages_big.shape, trainImages_big.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_BIG files created\n"
     ]
    }
   ],
   "source": [
    "writeNewFile(dir+newTestFile_big, testLabels_big, testImages_big)\n",
    "writeNewFile(dir+newTrainFile_big, trainLabels_big, trainImages_big)\n",
    "\n",
    "del testLabels_big, trainLabels_big, testImages_big, trainImages_big\n",
    "print('MNIST_BIG files created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Sentiment | Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdbpath = 'imdb/imdb.npz'\n",
    "with np.load(imdbpath, allow_pickle=True) as f:\n",
    "        x_train, labels_train = f['x_train'], f['y_train']\n",
    "        x_test, labels_test = f['x_test'], f['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "152\n",
      "25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x_test))\n",
    "print(len(x_test[0]))\n",
    "\n",
    "print(len(labels_test)) #binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def imdbCSV(file, label, data):\n",
    "    with open(file, 'w', newline='\\n') as myfile:\n",
    "        for i in range(len(data)):\n",
    "            wr = csv.writer(myfile)\n",
    "            data[i].insert(0, label[i])\n",
    "            wr.writerow(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdbCSV('imdb/imdb_train.csv', labels_train, x_train)\n",
    "imdbCSV('imdb/imdb_test.csv', labels_test, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston Housing Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhpath = 'boston_housing/boston_housing.npz'\n",
    "test_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(bhpath, allow_pickle=True) as f:\n",
    "    x = f['x']\n",
    "    y = f['y']\n",
    "\n",
    "x_train = np.array(x[:int(len(x) * (1 - test_split))])\n",
    "y_train = np.array(y[:int(len(x) * (1 - test_split))])\n",
    "x_test = np.array(x[int(len(x) * (1 - test_split)):])\n",
    "y_test = np.array(y[int(len(x) * (1 - test_split)):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhCSV(file, data):\n",
    "    with open(file, 'w', newline='\\n') as f:\n",
    "        for i in range(len(data)):\n",
    "            wr = csv.writer(f)\n",
    "            wr.writerow(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.column_stack([y_test, x_test])\n",
    "train = np.column_stack([y_train, x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhCSV('boston_housing/boston_housing_test.csv',test)\n",
    "bhCSV('boston_housing/boston_housing_train.csv',train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath = 'reuters/reuters.npz'\n",
    "test_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  8  3 ... 13 30 25]\n"
     ]
    }
   ],
   "source": [
    "with np.load(rpath, allow_pickle=True) as f:\n",
    "        xs, labels = f['x'], f['y']\n",
    "\n",
    "idx = int(len(xs) * (1 - test_split))\n",
    "x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
    "x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reutersCSV(file, label, data):\n",
    "    with open(file, 'w', newline='\\n') as myfile:\n",
    "        for i in range(len(data)):\n",
    "            wr = csv.writer(myfile)\n",
    "            data[i].insert(0, label[i])\n",
    "            wr.writerow(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersCSV('reuters/reuters_test.csv', y_test, x_test)\n",
    "reutersCSV('reuters/reuters_train.csv', y_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
